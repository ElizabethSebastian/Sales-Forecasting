{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "from  datetime import datetime, timedelta\n",
    "\n",
    "# Data preprocessing\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Import DataFrames.\n",
    "sales=pd.read_csv('sales_train_evaluation.csv')\n",
    "#sales=reduce_mem_usage(sales)\n",
    "#sales=pd.read_csv('sales_train_validation.csv')\n",
    "#sales=reduce_mem_usage(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "price=pd.read_csv('sell_prices.csv')\n",
    "price=reduce_mem_usage(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "calendar=pd.read_csv('calendar.csv')\n",
    "calendar=reduce_mem_usage(calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  2.09 Mb (84.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "sample=pd.read_csv('sample_submission.csv')\n",
    "sample=reduce_mem_usage(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 3273.49 Mb (9.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "catcol = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "sales= pd.melt(sales, id_vars= catcol, \n",
    "               var_name='day', value_name='demand')\n",
    "sales = reduce_mem_usage(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_rows = [row for row in sample['id'] if 'evaluation' in row]\n",
    "test2 = sample[sample['id'].isin(test2_rows)]\n",
    "\n",
    "test2.columns = ['id', 'd_1942', 'd_1943', 'd_1944', 'd_1945', 'd_1946', 'd_1947', 'd_1948', 'd_1949', 'd_1950', 'd_1951', 'd_1952', 'd_1953', 'd_1954', 'd_1955', 'd_1956', 'd_1957', 'd_1958', 'd_1959', \n",
    "                  'd_1960', 'd_1961', 'd_1962', 'd_1963', 'd_1964', 'd_1965', 'd_1966', 'd_1967', 'd_1968', 'd_1969']\n",
    "\n",
    "product = sales[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n",
    "\n",
    "\n",
    "test2 = test2.merge(product, how = 'left', on = 'id')\n",
    "\n",
    "test2 = pd.melt(test2, \n",
    "                id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                var_name = 'day', \n",
    "                value_name = 'demand')\n",
    "\n",
    "sales['part'] = 'train'\n",
    "test2['part'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([sales,test2], axis = 0)\n",
    "import gc\n",
    "del sales,test2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>day</th>\n",
       "      <th>demand</th>\n",
       "      <th>part</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_evaluation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_evaluation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_evaluation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_evaluation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_evaluation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_evaluation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  day  demand   part  \n",
       "0       CA  d_1       0  train  \n",
       "1       CA  d_1       0  train  \n",
       "2       CA  d_1       0  train  \n",
       "3       CA  d_1       0  train  \n",
       "4       CA  d_1       0  train  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True,inplace=True)\n",
    "data = data.loc[30000000:]\n",
    "calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n",
    "data = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n",
    "data.drop(['d', 'day'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.merge(price, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n",
    "del calendar,price,product\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>demand</th>\n",
       "      <th>part</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOUSEHOLD_1_331_WI_3_evaluation</td>\n",
       "      <td>HOUSEHOLD_1_331</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2013-10-08</td>\n",
       "      <td>11337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.878906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOUSEHOLD_1_332_WI_3_evaluation</td>\n",
       "      <td>HOUSEHOLD_1_332</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2013-10-08</td>\n",
       "      <td>11337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOUSEHOLD_1_333_WI_3_evaluation</td>\n",
       "      <td>HOUSEHOLD_1_333</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2013-10-08</td>\n",
       "      <td>11337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.970703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOUSEHOLD_1_334_WI_3_evaluation</td>\n",
       "      <td>HOUSEHOLD_1_334</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>11</td>\n",
       "      <td>train</td>\n",
       "      <td>2013-10-08</td>\n",
       "      <td>11337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOUSEHOLD_1_335_WI_3_evaluation</td>\n",
       "      <td>HOUSEHOLD_1_335</td>\n",
       "      <td>HOUSEHOLD_1</td>\n",
       "      <td>HOUSEHOLD</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2013-10-08</td>\n",
       "      <td>11337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.968750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id          item_id      dept_id     cat_id  \\\n",
       "0  HOUSEHOLD_1_331_WI_3_evaluation  HOUSEHOLD_1_331  HOUSEHOLD_1  HOUSEHOLD   \n",
       "1  HOUSEHOLD_1_332_WI_3_evaluation  HOUSEHOLD_1_332  HOUSEHOLD_1  HOUSEHOLD   \n",
       "2  HOUSEHOLD_1_333_WI_3_evaluation  HOUSEHOLD_1_333  HOUSEHOLD_1  HOUSEHOLD   \n",
       "3  HOUSEHOLD_1_334_WI_3_evaluation  HOUSEHOLD_1_334  HOUSEHOLD_1  HOUSEHOLD   \n",
       "4  HOUSEHOLD_1_335_WI_3_evaluation  HOUSEHOLD_1_335  HOUSEHOLD_1  HOUSEHOLD   \n",
       "\n",
       "  store_id state_id  demand   part        date  wm_yr_wk event_name_1  \\\n",
       "0     WI_3       WI       0  train  2013-10-08     11337          NaN   \n",
       "1     WI_3       WI       0  train  2013-10-08     11337          NaN   \n",
       "2     WI_3       WI       0  train  2013-10-08     11337          NaN   \n",
       "3     WI_3       WI      11  train  2013-10-08     11337          NaN   \n",
       "4     WI_3       WI       0  train  2013-10-08     11337          NaN   \n",
       "\n",
       "  event_type_1 event_name_2 event_type_2  snap_CA  snap_TX  snap_WI  \\\n",
       "0          NaN          NaN          NaN        1        0        1   \n",
       "1          NaN          NaN          NaN        1        0        1   \n",
       "2          NaN          NaN          NaN        1        0        1   \n",
       "3          NaN          NaN          NaN        1        0        1   \n",
       "4          NaN          NaN          NaN        1        0        1   \n",
       "\n",
       "   sell_price  \n",
       "0    4.878906  \n",
       "1         NaN  \n",
       "2    2.970703  \n",
       "3    0.979980  \n",
       "4    8.968750  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               object\n",
       "item_id          object\n",
       "dept_id          object\n",
       "cat_id           object\n",
       "store_id         object\n",
       "state_id         object\n",
       "demand            int16\n",
       "part             object\n",
       "date             object\n",
       "wm_yr_wk          int16\n",
       "event_name_1     object\n",
       "event_type_1     object\n",
       "event_name_2     object\n",
       "event_type_2     object\n",
       "snap_CA            int8\n",
       "snap_TX            int8\n",
       "snap_WI            int8\n",
       "sell_price      float16\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "for feature in nan_features:\n",
    "    data[feature].fillna('unknown', inplace = True)\n",
    "\n",
    "cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "for feature in cat:\n",
    "    encoder = LabelEncoder()\n",
    "    data[feature] = encoder.fit_transform(data[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>demand</th>\n",
       "      <th>part</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOUSEHOLD_1_331_WI_3_evaluation</td>\n",
       "      <td>2326</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2013-10-08</td>\n",
       "      <td>11337</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.878906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOUSEHOLD_1_332_WI_3_evaluation</td>\n",
       "      <td>2327</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2013-10-08</td>\n",
       "      <td>11337</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOUSEHOLD_1_333_WI_3_evaluation</td>\n",
       "      <td>2328</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2013-10-08</td>\n",
       "      <td>11337</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.970703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOUSEHOLD_1_334_WI_3_evaluation</td>\n",
       "      <td>2329</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>train</td>\n",
       "      <td>2013-10-08</td>\n",
       "      <td>11337</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.979980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOUSEHOLD_1_335_WI_3_evaluation</td>\n",
       "      <td>2330</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>2013-10-08</td>\n",
       "      <td>11337</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.968750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  item_id  dept_id  cat_id  store_id  \\\n",
       "0  HOUSEHOLD_1_331_WI_3_evaluation     2326        5       2         9   \n",
       "1  HOUSEHOLD_1_332_WI_3_evaluation     2327        5       2         9   \n",
       "2  HOUSEHOLD_1_333_WI_3_evaluation     2328        5       2         9   \n",
       "3  HOUSEHOLD_1_334_WI_3_evaluation     2329        5       2         9   \n",
       "4  HOUSEHOLD_1_335_WI_3_evaluation     2330        5       2         9   \n",
       "\n",
       "   state_id  demand   part        date  wm_yr_wk  event_name_1  event_type_1  \\\n",
       "0         2       0  train  2013-10-08     11337            30             4   \n",
       "1         2       0  train  2013-10-08     11337            30             4   \n",
       "2         2       0  train  2013-10-08     11337            30             4   \n",
       "3         2      11  train  2013-10-08     11337            30             4   \n",
       "4         2       0  train  2013-10-08     11337            30             4   \n",
       "\n",
       "   event_name_2  event_type_2  snap_CA  snap_TX  snap_WI  sell_price  \n",
       "0             2             2        1        0        1    4.878906  \n",
       "1             2             2        1        0        1         NaN  \n",
       "2             2             2        1        0        1    2.970703  \n",
       "3             2             2        1        0        1    0.979980  \n",
       "4             2             2        1        0        1    8.968750  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               object\n",
       "item_id           int32\n",
       "dept_id           int32\n",
       "cat_id            int32\n",
       "store_id          int32\n",
       "state_id          int32\n",
       "demand            int16\n",
       "part             object\n",
       "date             object\n",
       "wm_yr_wk          int16\n",
       "event_name_1      int32\n",
       "event_type_1      int32\n",
       "event_name_2      int32\n",
       "event_type_2      int32\n",
       "snap_CA            int8\n",
       "snap_TX            int8\n",
       "snap_WI            int8\n",
       "sell_price      float16\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lag_t28'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n",
    "data['lag_t29'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(29))\n",
    "data['lag_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(30))\n",
    "data['rolling_mean_t7']   = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "data['rolling_std_t7']    = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "data['rolling_mean_t30']  = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1804.54 Mb (49.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rolling_mean_t90']  = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "data['rolling_mean_t180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "data['rolling_std_t30']   = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "data['rolling_skew_t30']  = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n",
    "data['rolling_kurt_t30']  = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) / (data['lag_price_t1'])\n",
    "data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n",
    "data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) / (data['rolling_price_max_t365'])\n",
    "data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n",
    "data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['week'] = data['date'].dt.week\n",
    "data['day'] = data['date'].dt.day\n",
    "data['dayofweek'] = data['date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weekend(arg):\n",
    "    if arg==5 or arg==6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "data['isweekend'] = data['dayofweek'].apply(weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['revenue'] = data['demand'] * data['sell_price']\n",
    "data['lag_revenue_t1'] = data.groupby(['id'])['revenue'].transform(lambda x: x.shift(28))\n",
    "data['rolling_revenue_std_t28'] = data.groupby(['id'])['lag_revenue_t1'].transform(lambda x: x.rolling(28).std())\n",
    "data['rolling_revenue_mean_t28'] = data.groupby(['id'])['lag_revenue_t1'].transform(lambda x: x.rolling(28).mean())\n",
    "data.drop(['revenue'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2692.48 Mb (45.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "features = ['item_id', 'cat_id', 'state_id', 'year', 'month', 'week', 'day', 'dayofweek', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', \n",
    "            'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29', 'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', \n",
    "            'rolling_mean_t180', 'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30', 'rolling_skew_t30', 'rolling_kurt_t30',\n",
    "            'isweekend','lag_revenue_t1','rolling_revenue_std_t28','rolling_revenue_mean_t28'\n",
    "        ]\n",
    "data = reduce_mem_usage(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data[data['part'] == 'train']\n",
    "y_train = x_train['demand']\n",
    "x_val = data[(data['date'] > '2016-04-24') & (data['part'] == 'train')]\n",
    "y_val = x_val['demand']\n",
    "test = data[data['date'] > '2016-04-24']\n",
    "test.loc[test['part']=='train','id'] = test.loc[test['part']=='train','id'].str.replace('_evaluation','_validation')\n",
    "x_train.drop(['demand','part',],inplace=True,axis=1)\n",
    "x_val.drop(['demand','part',],inplace=True,axis=1)\n",
    "test.drop(['demand','part',],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import Dataset,train,plot_importance\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': 'rmse',\n",
    "    'objective': 'regression',\n",
    "    'n_jobs': -1,\n",
    "    'seed': 236,\n",
    "    'learning_rate': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = x_train['dept_id'].unique()\n",
    "category2 = x_train['store_id'].unique()\n",
    "def dataset(categor,categor2):\n",
    "    tindex = x_train[(x_train['dept_id']==categor) & (x_train['store_id']==categor2)].index.values\n",
    "    vindex = x_val[(x_val['dept_id']==categor)&(x_val['store_id']==categor2)].index.values\n",
    "    \n",
    "    x_t,x_v,y_t,y_v,t =  x_train[(x_train['dept_id']==categor) & (x_train['store_id']==categor2)],\n",
    "                        x_val[(x_val['dept_id']==categor)&(x_val['store_id']==categor2)],\n",
    "                        y_train.loc[tindex], y_val.loc[vindex],\n",
    "                        test[(test['dept_id']==categor) &(test['store_id']==categor2)]\n",
    "    \n",
    "    x_train.drop(tindex,axis=0,inplace=True)\n",
    "    x_val.drop(vindex,axis=0,inplace=True)\n",
    "    test.drop(test[(test['dept_id']==categor) &(test['store_id']==categor2)].index.values,axis=0,inplace=True)\n",
    "    \n",
    "    return x_t,x_v,y_t,y_v,t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.084502 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4115\n",
      "[LightGBM] [Info] Number of data points in the train set: 509332, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.040231\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.19306\tvalid_1's rmse: 1.26201\n",
      "[2000]\ttraining's rmse: 1.08074\tvalid_1's rmse: 1.11463\n",
      "[3000]\ttraining's rmse: 1.00408\tvalid_1's rmse: 1.03622\n",
      "[4000]\ttraining's rmse: 0.944439\tvalid_1's rmse: 0.975225\n",
      "[5000]\ttraining's rmse: 0.896005\tvalid_1's rmse: 0.925215\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.896005\tvalid_1's rmse: 0.925215\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.087053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4057\n",
      "[LightGBM] [Info] Number of data points in the train set: 509124, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.316308\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.3611\tvalid_1's rmse: 1.34702\n",
      "[2000]\ttraining's rmse: 1.25268\tvalid_1's rmse: 1.22586\n",
      "[3000]\ttraining's rmse: 1.1725\tvalid_1's rmse: 1.15849\n",
      "[4000]\ttraining's rmse: 1.11196\tvalid_1's rmse: 1.10556\n",
      "[5000]\ttraining's rmse: 1.06166\tvalid_1's rmse: 1.05949\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.06166\tvalid_1's rmse: 1.05949\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.090146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4062\n",
      "[LightGBM] [Info] Number of data points in the train set: 509124, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.194220\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.30821\tvalid_1's rmse: 1.42957\n",
      "[2000]\ttraining's rmse: 1.20242\tvalid_1's rmse: 1.30527\n",
      "[3000]\ttraining's rmse: 1.12851\tvalid_1's rmse: 1.21874\n",
      "[4000]\ttraining's rmse: 1.06936\tvalid_1's rmse: 1.1553\n",
      "[5000]\ttraining's rmse: 1.02302\tvalid_1's rmse: 1.10495\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.02302\tvalid_1's rmse: 1.10495\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.085160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4304\n",
      "[LightGBM] [Info] Number of data points in the train set: 509124, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 2.497523\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 2.15357\tvalid_1's rmse: 2.16743\n",
      "[2000]\ttraining's rmse: 1.91401\tvalid_1's rmse: 1.94468\n",
      "[3000]\ttraining's rmse: 1.75892\tvalid_1's rmse: 1.79109\n",
      "[4000]\ttraining's rmse: 1.64575\tvalid_1's rmse: 1.67783\n",
      "[5000]\ttraining's rmse: 1.55342\tvalid_1's rmse: 1.58495\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.55342\tvalid_1's rmse: 1.58495\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.102685 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3843\n",
      "[LightGBM] [Info] Number of data points in the train set: 509124, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.632378\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 0.870918\tvalid_1's rmse: 0.955792\n",
      "[2000]\ttraining's rmse: 0.818249\tvalid_1's rmse: 0.889754\n",
      "[3000]\ttraining's rmse: 0.778023\tvalid_1's rmse: 0.841736\n",
      "[4000]\ttraining's rmse: 0.745485\tvalid_1's rmse: 0.799997\n",
      "[5000]\ttraining's rmse: 0.716618\tvalid_1's rmse: 0.767336\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.716618\tvalid_1's rmse: 0.767336\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082406 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4186\n",
      "[LightGBM] [Info] Number of data points in the train set: 509124, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.291090\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.41382\tvalid_1's rmse: 1.36653\n",
      "[2000]\ttraining's rmse: 1.2715\tvalid_1's rmse: 1.24397\n",
      "[3000]\ttraining's rmse: 1.17991\tvalid_1's rmse: 1.16305\n",
      "[4000]\ttraining's rmse: 1.10775\tvalid_1's rmse: 1.09332\n",
      "[5000]\ttraining's rmse: 1.0472\tvalid_1's rmse: 1.02917\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.0472\tvalid_1's rmse: 1.02917\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046546 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4283\n",
      "[LightGBM] [Info] Number of data points in the train set: 509124, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.333479\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.73037\tvalid_1's rmse: 1.40022\n",
      "[2000]\ttraining's rmse: 1.54929\tvalid_1's rmse: 1.29868\n",
      "[3000]\ttraining's rmse: 1.43864\tvalid_1's rmse: 1.2181\n",
      "[4000]\ttraining's rmse: 1.35407\tvalid_1's rmse: 1.15004\n",
      "[5000]\ttraining's rmse: 1.28876\tvalid_1's rmse: 1.09409\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.28876\tvalid_1's rmse: 1.09409\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.052529 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4269\n",
      "[LightGBM] [Info] Number of data points in the train set: 509124, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.380143\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.41745\tvalid_1's rmse: 1.45219\n",
      "[2000]\ttraining's rmse: 1.27924\tvalid_1's rmse: 1.31923\n",
      "[3000]\ttraining's rmse: 1.18753\tvalid_1's rmse: 1.22172\n",
      "[4000]\ttraining's rmse: 1.11726\tvalid_1's rmse: 1.14676\n",
      "[5000]\ttraining's rmse: 1.06011\tvalid_1's rmse: 1.08652\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.06011\tvalid_1's rmse: 1.08652\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4026\n",
      "[LightGBM] [Info] Number of data points in the train set: 509124, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.959741\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.0955\tvalid_1's rmse: 1.13448\n",
      "[2000]\ttraining's rmse: 1.01556\tvalid_1's rmse: 1.0508\n",
      "[3000]\ttraining's rmse: 0.959021\tvalid_1's rmse: 0.996896\n",
      "[4000]\ttraining's rmse: 0.912554\tvalid_1's rmse: 0.95177\n",
      "[5000]\ttraining's rmse: 0.873064\tvalid_1's rmse: 0.910596\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.873064\tvalid_1's rmse: 0.910596\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051550 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4268\n",
      "[LightGBM] [Info] Number of data points in the train set: 509124, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.493807\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.52973\tvalid_1's rmse: 1.54464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\ttraining's rmse: 1.35714\tvalid_1's rmse: 1.36388\n",
      "[3000]\ttraining's rmse: 1.24842\tvalid_1's rmse: 1.2588\n",
      "[4000]\ttraining's rmse: 1.1692\tvalid_1's rmse: 1.18511\n",
      "[5000]\ttraining's rmse: 1.10416\tvalid_1's rmse: 1.11677\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.10416\tvalid_1's rmse: 1.11677\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3611\n",
      "[LightGBM] [Info] Number of data points in the train set: 493370, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.225537\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 0.475181\tvalid_1's rmse: 0.542332\n",
      "[2000]\ttraining's rmse: 0.446482\tvalid_1's rmse: 0.502247\n",
      "[3000]\ttraining's rmse: 0.425893\tvalid_1's rmse: 0.472902\n",
      "[4000]\ttraining's rmse: 0.408206\tvalid_1's rmse: 0.447882\n",
      "[5000]\ttraining's rmse: 0.393755\tvalid_1's rmse: 0.428529\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.393755\tvalid_1's rmse: 0.428529\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058673 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3658\n",
      "[LightGBM] [Info] Number of data points in the train set: 492855, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.383819\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 0.664779\tvalid_1's rmse: 0.696712\n",
      "[2000]\ttraining's rmse: 0.626294\tvalid_1's rmse: 0.657713\n",
      "[3000]\ttraining's rmse: 0.597414\tvalid_1's rmse: 0.623788\n",
      "[4000]\ttraining's rmse: 0.573063\tvalid_1's rmse: 0.595867\n",
      "[5000]\ttraining's rmse: 0.551183\tvalid_1's rmse: 0.573398\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.551183\tvalid_1's rmse: 0.573398\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3952\n",
      "[LightGBM] [Info] Number of data points in the train set: 492855, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.497035\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 0.797857\tvalid_1's rmse: 0.924704\n",
      "[2000]\ttraining's rmse: 0.743513\tvalid_1's rmse: 0.852966\n",
      "[3000]\ttraining's rmse: 0.705168\tvalid_1's rmse: 0.803726\n",
      "[4000]\ttraining's rmse: 0.6736\tvalid_1's rmse: 0.767712\n",
      "[5000]\ttraining's rmse: 0.646836\tvalid_1's rmse: 0.734111\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.646836\tvalid_1's rmse: 0.734111\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.063727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3955\n",
      "[LightGBM] [Info] Number of data points in the train set: 492855, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.607797\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 0.897837\tvalid_1's rmse: 0.971528\n",
      "[2000]\ttraining's rmse: 0.835705\tvalid_1's rmse: 0.90424\n",
      "[3000]\ttraining's rmse: 0.790176\tvalid_1's rmse: 0.848974\n",
      "[4000]\ttraining's rmse: 0.753268\tvalid_1's rmse: 0.809162\n",
      "[5000]\ttraining's rmse: 0.72168\tvalid_1's rmse: 0.776131\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.72168\tvalid_1's rmse: 0.776131\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.066799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3514\n",
      "[LightGBM] [Info] Number of data points in the train set: 492855, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.225180\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 0.505786\tvalid_1's rmse: 0.538635\n",
      "[2000]\ttraining's rmse: 0.470629\tvalid_1's rmse: 0.51017\n",
      "[3000]\ttraining's rmse: 0.447907\tvalid_1's rmse: 0.489052\n",
      "[4000]\ttraining's rmse: 0.429866\tvalid_1's rmse: 0.470574\n",
      "[5000]\ttraining's rmse: 0.414694\tvalid_1's rmse: 0.453538\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.414694\tvalid_1's rmse: 0.453538\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3937\n",
      "[LightGBM] [Info] Number of data points in the train set: 492855, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.289225\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 0.57928\tvalid_1's rmse: 0.624573\n",
      "[2000]\ttraining's rmse: 0.54285\tvalid_1's rmse: 0.585278\n",
      "[3000]\ttraining's rmse: 0.51608\tvalid_1's rmse: 0.556607\n",
      "[4000]\ttraining's rmse: 0.494238\tvalid_1's rmse: 0.529898\n",
      "[5000]\ttraining's rmse: 0.476243\tvalid_1's rmse: 0.509046\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.476243\tvalid_1's rmse: 0.509046\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3753\n",
      "[LightGBM] [Info] Number of data points in the train set: 492855, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.335070\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 0.614907\tvalid_1's rmse: 0.643929\n",
      "[2000]\ttraining's rmse: 0.579541\tvalid_1's rmse: 0.608055\n",
      "[3000]\ttraining's rmse: 0.553138\tvalid_1's rmse: 0.578278\n",
      "[4000]\ttraining's rmse: 0.52994\tvalid_1's rmse: 0.552048\n",
      "[5000]\ttraining's rmse: 0.511107\tvalid_1's rmse: 0.530627\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.511107\tvalid_1's rmse: 0.530627\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.061339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3661\n",
      "[LightGBM] [Info] Number of data points in the train set: 492855, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.287442\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 0.54809\tvalid_1's rmse: 0.590982\n",
      "[2000]\ttraining's rmse: 0.514828\tvalid_1's rmse: 0.552278\n",
      "[3000]\ttraining's rmse: 0.489607\tvalid_1's rmse: 0.521964\n",
      "[4000]\ttraining's rmse: 0.469742\tvalid_1's rmse: 0.497589\n",
      "[5000]\ttraining's rmse: 0.453082\tvalid_1's rmse: 0.478004\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.453082\tvalid_1's rmse: 0.478004\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.059153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3727\n",
      "[LightGBM] [Info] Number of data points in the train set: 492855, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.282215\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 0.554167\tvalid_1's rmse: 0.61241\n",
      "[2000]\ttraining's rmse: 0.522751\tvalid_1's rmse: 0.576171\n",
      "[3000]\ttraining's rmse: 0.498947\tvalid_1's rmse: 0.550926\n",
      "[4000]\ttraining's rmse: 0.479573\tvalid_1's rmse: 0.529234\n",
      "[5000]\ttraining's rmse: 0.462972\tvalid_1's rmse: 0.511287\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.462972\tvalid_1's rmse: 0.511287\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.091144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3647\n",
      "[LightGBM] [Info] Number of data points in the train set: 492855, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.239182\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's rmse: 0.492866\tvalid_1's rmse: 0.523162\n",
      "[2000]\ttraining's rmse: 0.460643\tvalid_1's rmse: 0.487005\n",
      "[3000]\ttraining's rmse: 0.436343\tvalid_1's rmse: 0.457979\n",
      "[4000]\ttraining's rmse: 0.41778\tvalid_1's rmse: 0.436976\n",
      "[5000]\ttraining's rmse: 0.40212\tvalid_1's rmse: 0.419797\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.40212\tvalid_1's rmse: 0.419797\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.024843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3988\n",
      "[LightGBM] [Info] Number of data points in the train set: 206928, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.982491\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.12664\tvalid_1's rmse: 1.24097\n",
      "[2000]\ttraining's rmse: 0.977942\tvalid_1's rmse: 1.07736\n",
      "[3000]\ttraining's rmse: 0.8797\tvalid_1's rmse: 0.971346\n",
      "[4000]\ttraining's rmse: 0.805932\tvalid_1's rmse: 0.894111\n",
      "[5000]\ttraining's rmse: 0.745723\tvalid_1's rmse: 0.82057\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.745723\tvalid_1's rmse: 0.82057\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.023362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3938\n",
      "[LightGBM] [Info] Number of data points in the train set: 206712, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.526143\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.54974\tvalid_1's rmse: 1.84743\n",
      "[2000]\ttraining's rmse: 1.3535\tvalid_1's rmse: 1.63343\n",
      "[3000]\ttraining's rmse: 1.2218\tvalid_1's rmse: 1.47938\n",
      "[4000]\ttraining's rmse: 1.12559\tvalid_1's rmse: 1.36138\n",
      "[5000]\ttraining's rmse: 1.04453\tvalid_1's rmse: 1.26481\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.04453\tvalid_1's rmse: 1.26481\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.028198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3927\n",
      "[LightGBM] [Info] Number of data points in the train set: 206712, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.745985\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.85846\tvalid_1's rmse: 1.9679\n",
      "[2000]\ttraining's rmse: 1.62671\tvalid_1's rmse: 1.72492\n",
      "[3000]\ttraining's rmse: 1.47551\tvalid_1's rmse: 1.56297\n",
      "[4000]\ttraining's rmse: 1.35629\tvalid_1's rmse: 1.42112\n",
      "[5000]\ttraining's rmse: 1.2568\tvalid_1's rmse: 1.32546\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.2568\tvalid_1's rmse: 1.32546\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4152\n",
      "[LightGBM] [Info] Number of data points in the train set: 206712, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.898709\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 2.02409\tvalid_1's rmse: 2.17684\n",
      "[2000]\ttraining's rmse: 1.75726\tvalid_1's rmse: 1.90626\n",
      "[3000]\ttraining's rmse: 1.58092\tvalid_1's rmse: 1.73683\n",
      "[4000]\ttraining's rmse: 1.45056\tvalid_1's rmse: 1.59827\n",
      "[5000]\ttraining's rmse: 1.34376\tvalid_1's rmse: 1.47675\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.34376\tvalid_1's rmse: 1.47675\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3814\n",
      "[LightGBM] [Info] Number of data points in the train set: 206712, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.966277\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.14577\tvalid_1's rmse: 1.28137\n",
      "[2000]\ttraining's rmse: 1.00888\tvalid_1's rmse: 1.13768\n",
      "[3000]\ttraining's rmse: 0.919246\tvalid_1's rmse: 1.02823\n",
      "[4000]\ttraining's rmse: 0.849788\tvalid_1's rmse: 0.953375\n",
      "[5000]\ttraining's rmse: 0.789797\tvalid_1's rmse: 0.881772\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.789797\tvalid_1's rmse: 0.881772\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3980\n",
      "[LightGBM] [Info] Number of data points in the train set: 206712, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.986208\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.2634\tvalid_1's rmse: 1.31932\n",
      "[2000]\ttraining's rmse: 1.10146\tvalid_1's rmse: 1.16095\n",
      "[3000]\ttraining's rmse: 0.994635\tvalid_1's rmse: 1.05189\n",
      "[4000]\ttraining's rmse: 0.913488\tvalid_1's rmse: 0.96672\n",
      "[5000]\ttraining's rmse: 0.851322\tvalid_1's rmse: 0.898771\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.851322\tvalid_1's rmse: 0.898771\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4009\n",
      "[LightGBM] [Info] Number of data points in the train set: 206712, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.186922\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.32144\tvalid_1's rmse: 1.45718\n",
      "[2000]\ttraining's rmse: 1.15046\tvalid_1's rmse: 1.29563\n",
      "[3000]\ttraining's rmse: 1.03806\tvalid_1's rmse: 1.17763\n",
      "[4000]\ttraining's rmse: 0.951536\tvalid_1's rmse: 1.07913\n",
      "[5000]\ttraining's rmse: 0.884339\tvalid_1's rmse: 1.00883\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.884339\tvalid_1's rmse: 1.00883\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027712 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4049\n",
      "[LightGBM] [Info] Number of data points in the train set: 206712, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.217796\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.30921\tvalid_1's rmse: 1.54898\n",
      "[2000]\ttraining's rmse: 1.11941\tvalid_1's rmse: 1.32872\n",
      "[3000]\ttraining's rmse: 1.00172\tvalid_1's rmse: 1.19255\n",
      "[4000]\ttraining's rmse: 0.914592\tvalid_1's rmse: 1.09376\n",
      "[5000]\ttraining's rmse: 0.845094\tvalid_1's rmse: 1.01856\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.845094\tvalid_1's rmse: 1.01856\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019259 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3954\n",
      "[LightGBM] [Info] Number of data points in the train set: 206712, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.332182\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.35106\tvalid_1's rmse: 1.43356\n",
      "[2000]\ttraining's rmse: 1.18618\tvalid_1's rmse: 1.27979\n",
      "[3000]\ttraining's rmse: 1.07899\tvalid_1's rmse: 1.16807\n",
      "[4000]\ttraining's rmse: 0.99561\tvalid_1's rmse: 1.08655\n",
      "[5000]\ttraining's rmse: 0.926377\tvalid_1's rmse: 1.01312\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.926377\tvalid_1's rmse: 1.01312\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4103\n",
      "[LightGBM] [Info] Number of data points in the train set: 206712, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.493455\n",
      "Training until validation scores don't improve for 40 rounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's rmse: 1.48731\tvalid_1's rmse: 1.69604\n",
      "[2000]\ttraining's rmse: 1.25646\tvalid_1's rmse: 1.45606\n",
      "[3000]\ttraining's rmse: 1.11884\tvalid_1's rmse: 1.29493\n",
      "[4000]\ttraining's rmse: 1.0179\tvalid_1's rmse: 1.16964\n",
      "[5000]\ttraining's rmse: 0.938146\tvalid_1's rmse: 1.0693\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.938146\tvalid_1's rmse: 1.0693\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045453 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4127\n",
      "[LightGBM] [Info] Number of data points in the train set: 381284, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.985336\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.18913\tvalid_1's rmse: 1.36017\n",
      "[2000]\ttraining's rmse: 1.07589\tvalid_1's rmse: 1.22983\n",
      "[3000]\ttraining's rmse: 0.998099\tvalid_1's rmse: 1.13585\n",
      "[4000]\ttraining's rmse: 0.936896\tvalid_1's rmse: 1.06322\n",
      "[5000]\ttraining's rmse: 0.887452\tvalid_1's rmse: 0.999362\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.887452\tvalid_1's rmse: 0.999362\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4096\n",
      "[LightGBM] [Info] Number of data points in the train set: 380886, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.196747\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.35244\tvalid_1's rmse: 1.37441\n",
      "[2000]\ttraining's rmse: 1.22662\tvalid_1's rmse: 1.26544\n",
      "[3000]\ttraining's rmse: 1.13862\tvalid_1's rmse: 1.18449\n",
      "[4000]\ttraining's rmse: 1.07104\tvalid_1's rmse: 1.11979\n",
      "[5000]\ttraining's rmse: 1.01295\tvalid_1's rmse: 1.05903\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.01295\tvalid_1's rmse: 1.05903\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042860 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3826\n",
      "[LightGBM] [Info] Number of data points in the train set: 380886, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.593296\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 0.889682\tvalid_1's rmse: 1.2569\n",
      "[2000]\ttraining's rmse: 0.793029\tvalid_1's rmse: 1.12642\n",
      "[3000]\ttraining's rmse: 0.721601\tvalid_1's rmse: 1.03054\n",
      "[4000]\ttraining's rmse: 0.666679\tvalid_1's rmse: 0.947036\n",
      "[5000]\ttraining's rmse: 0.619646\tvalid_1's rmse: 0.875749\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.619646\tvalid_1's rmse: 0.875749\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4421\n",
      "[LightGBM] [Info] Number of data points in the train set: 380886, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 1.579016\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.59539\tvalid_1's rmse: 1.61877\n",
      "[2000]\ttraining's rmse: 1.44399\tvalid_1's rmse: 1.47947\n",
      "[3000]\ttraining's rmse: 1.34182\tvalid_1's rmse: 1.37018\n",
      "[4000]\ttraining's rmse: 1.26363\tvalid_1's rmse: 1.28414\n",
      "[5000]\ttraining's rmse: 1.19451\tvalid_1's rmse: 1.21083\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 1.19451\tvalid_1's rmse: 1.21083\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3919\n",
      "[LightGBM] [Info] Number of data points in the train set: 380886, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.752309\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.0078\tvalid_1's rmse: 1.10852\n",
      "[2000]\ttraining's rmse: 0.928369\tvalid_1's rmse: 1.02672\n",
      "[3000]\ttraining's rmse: 0.870404\tvalid_1's rmse: 0.964523\n",
      "[4000]\ttraining's rmse: 0.822284\tvalid_1's rmse: 0.911707\n",
      "[5000]\ttraining's rmse: 0.783447\tvalid_1's rmse: 0.867812\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's rmse: 0.783447\tvalid_1's rmse: 0.867812\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4351\n",
      "[LightGBM] [Info] Number of data points in the train set: 380886, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 0.852793\n",
      "Training until validation scores don't improve for 40 rounds\n",
      "[1000]\ttraining's rmse: 1.08971\tvalid_1's rmse: 1.05237\n",
      "[2000]\ttraining's rmse: 0.991215\tvalid_1's rmse: 0.975098\n",
      "[3000]\ttraining's rmse: 0.9224\tvalid_1's rmse: 0.916231\n",
      "[4000]\ttraining's rmse: 0.867164\tvalid_1's rmse: 0.864082\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "TEST = None\n",
    "\n",
    "\n",
    "for i in category:\n",
    "    for j in category2:\n",
    "        x_t,x_v,y_t,y_v,t = dataset(i,j)\n",
    "        train_set = Dataset(x_t[features], y_t)\n",
    "        val_set = Dataset(x_v[features], y_v)\n",
    "        del x_t, y_t\n",
    "        gc.collect()\n",
    "\n",
    "        model = train(params, train_set, num_boost_round = 5000, early_stopping_rounds = 40, \n",
    "                      valid_sets = [train_set, val_set], verbose_eval = 1000)\n",
    "        y_pred = model.predict(t[features])\n",
    "\n",
    "        t['demand'] = y_pred\n",
    "\n",
    "        TEST = pd.concat([TEST,t],axis=0)\n",
    "        del x_v,y_v,t,train_set,val_set,y_pred,model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = TEST[TEST['id'].apply(lambda x: \"validation\" in x)][['id', 'date', 'sales']]\n",
    "predictions2 = TEST[TEST['id'].apply(lambda x: \"evaluation\" in x)][['id', 'date', 'sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1 = pd.pivot(predictions1, index = 'id', columns = 'date', values = 'sales').reset_index()\n",
    "prediction2 = pd.pivot(predictions2, index = 'id', columns = 'date', values = 'sales').reset_index()\n",
    "prediction1.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "prediction2.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat([prediction1,prediction2],axis=0)\n",
    "predictions.to_csv(\"submissionstoreanddeptwise.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=pd.read_csv(\"submissionstoreanddeptwise.csv\")\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip submissionstoreanddeptwise.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
